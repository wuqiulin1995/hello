Solid State Drives (SSDs) are the dominant form of secondary storage in web-scale data centers, hyperscale infrastructures and shared storage arrays. While their internal parallelism has been enough to outperform spinning disks, SSDs have a widely accepted and well documented Achilles¡¯ heel: unpredictable, tail latencies. This is mainly caused by two factors: (i) the need for performing background garbage collection (GC), and more importantly, (ii) the latency of a given I/O being determined by the order in which it is scheduled. Programming operations (write and erase) exhibit an order of magnitude higher base latencies than read operations. Therefore, reads are highly disturbed by I/O collisions on the physical media. This problem only aggravates in multi-tenancy setups, where a single SSD is shared among several tenants, each implementing its own scheduling on a different workload.
A straightforward solution is to use dedicated devices for each tenant. However, the increasing density of non-volatile memories and the associated cost makes this an expensive solution. Moreover, the increasing parallelism provided by more powerful SSD-controllers makes this solution also inefficient. Indeed, next generation SSDs support +1M IOPS in a single device. Another solution is to partition the device into several namespaces and dedicate them to each tenant. This tackles the cost issue and allows to take advantage of the device parallelism. However, namespaces, as defined in current protocols such as SCSI and more recently NVMe, are not guaranteed to be mapped onto physical parallel units in the device. This means that I/Os stemming from different namespaces could be addressed to the same physical die, therefore colliding when they reach the media. A different way of partitioning the SSD is by logically tagging I/O streams and letting the device doing the physical separation. The problem is that current multi-stream solutions are limited in the number of streams exposed to the host. More importantly, the streams are static and do not resemble the device¡¯s parallel units either. Partitioning is then best-effort and is conditioned by the device¡¯s embedded data placement strategy.
In general, the problem is that current abstractions presented to the host by the OS do not capture the parallelism intrinsic in
the SSD, thus making it impossible to guarantee that applications will not disturb each other on a multi-tenant environment. In
this paper, we describe how I/O isolation can be achieved with Open-Channel SSDs without modifying current applications. We describe our architecture and report evaluation results.